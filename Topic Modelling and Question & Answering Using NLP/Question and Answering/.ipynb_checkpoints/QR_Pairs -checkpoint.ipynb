{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import os\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNum(text):\n",
    "    numReg = re.compile(r'^(\\d+\\.?)')\n",
    "    for i in range(len(text)-1):\n",
    "        text[i] = numReg.sub('',text[i])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readandclean(file):\n",
    "    file = open(file,'r',errors = 'ignore')\n",
    "    raw = file.read()\n",
    "    raw = raw.lower()\n",
    "    raw = raw.replace('\\t','')\n",
    "    raw = raw.replace('ï¿½','')\n",
    "    raw = raw.split('\\n')\n",
    "    raw = removeNum(raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdict(key, values):\n",
    "    qrzip = zip(key, values)\n",
    "    qr_pairs = dict(qrzip)\n",
    "    return qr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response,base_corpus):\n",
    "    qr_response = ''\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(base_corpus)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf == 0):\n",
    "        qr_response=qr_response + \"I am sorry! I don't understand you\"\n",
    "        return qr_response\n",
    "    else:\n",
    "        qr_response = qr_response+base_corpus[idx]\n",
    "        return qr_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = readandclean('../FAQs/Questions.txt')\n",
    "answers = readandclean('../FAQs/Answers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_pairs = createdict(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansques(question):\n",
    "    #global word_tokens\n",
    "    q = open(question,'r')\n",
    "    questions_raw = q.read()\n",
    "    questions_raw = questions_raw.replace('\\t','')\n",
    "    questions_raw = questions_raw.split('\\n')\n",
    "    questions_raw = removeNum(questions_raw)\n",
    "    for i in questions_raw:\n",
    "        if(i == ''):\n",
    "            questions_raw.remove(i)\n",
    "    print(questions_raw)\n",
    "    results = open('questions_file.txt','w')\n",
    "    for i in questions_raw:\n",
    "        questions.append(i)\n",
    "        results.write(qr_pairs[response(i,questions)] + '\\n')\n",
    "        print(qr_pairs[response(i,questions)])\n",
    "        questions.remove(i)\n",
    "    results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(questionfile):\n",
    "    questions = readandclean('../FAQs/Questions.txt')\n",
    "    answers = readandclean('../FAQs/Answers.txt')\n",
    "    qr_pairs = createdict(questions, answers)\n",
    "    ansques(questionfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what cash crop does ghana export?', 'which region in ghana is the largest?', 'who celebrates homowo in Ghana?', 'does fifa have more countries than the united nations?', 'does fifa have more countries than the un?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elementary/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocoa\n",
      "ten\n",
      "ga\n",
      "yes it does.\n",
      "fifa has 211 national associatons\n"
     ]
    }
   ],
   "source": [
    "main(sys.arg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
